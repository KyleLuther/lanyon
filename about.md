---
layout: page
title: About
---
<p align="center">
  <img src="/assets/images/circular_profile.png" width="200" />
</p>

I'm a physics graduate student at Princeton, advised by  Sebastian Seung. My research interests lie at the intersection of deep learning, computer vision, and computational neuroscience.

<!-- My main theoretical focus is in biologically plausible unsupervised learning. Can we take inspiration from the visual cortex to learn representations of visual scenes that are generally useful for downstream tasks such as object recognition? The core problem is invariances. Can we find a learning rule that handles this?

I've also worked on a related, but more empirical question: How is the visual cortex wired together? After a small volume* of visual cortex is imaged by electron microscopes, convolutional networks scan through the volume and label neurons and their synapses, resulting in a (partial) connectivity graph, the "connectome". Training networks to do this with sufficient accuracy is easier said than done however... -->
